#
## Klasyfikator KNN - k-najbliższych sąsiadów
- Nie ma procesu uczenia (procesu adaptacji wag pomiędzy warstwami)
- Ale ma dane uczące się (X, d), gdzie X to dane punkty, d to klasyfikacja
- Klasyfikator działa w trybie testującym wykorzystując bezpośrednio dane uczące

### KNN dzieli się na
- **Ostry** - Testowanie wektorem $X_t$
  1. Obliczanie odległości $X_t$ od danych $X_i$, czyli obliczanie normy $d(X_t, X_i) = ||X_t - X_i||$
  2. Wybiera się k-najbliższych sąsiadów spośród $X_i$ i sprawdza ich przynależność klasową
  3. $X_t$ przydziela się do klasy większościowej. 
- **Rozmyty** - Dodatkowo są wagi $w_i$
  1. Obliczanie odległości X_t od danych X_i, czyli obliczanie normy $d(X_t, X_i) = ||X_t - X_i||$
  2. Wybiera się k-najbliższych sąsiadów spośród X_i i sprawdza ich przynależność klasową
  3. Określić współczynnik przynależności rozmytej każdego sąsiada do $X_t$, często $\mu(X_t, X_i) = \frac{1}{||X_t - X_i||}$
  4. Zsumować wartości $\mu(X_i)$ dla każdej klasy i przypisanie $X_t$ klasy o najwyższym współczynniku.

### Dylematy związane z tym modelem
- wybór $k$, typowo 3, 5; powyżej 9ciu zazwyczaj się nie bierze
- wybór normy $||X_t - X_i||$
- (w przypadku rozmytego) definicja $\mu(\dots)$
  
## Model klasyfikatora Bayesa - klasyfikator probabilystyczny

x $\rightarrow$ [ klasyfikator bayesa ] $\rightarrow$ y - binarny

X może być i rzeczywistoliczbowy i binarny

### I love math
$$ P(A), P(\bar{A}) = 1 - P(A) $$
$$ P(A \cap B) = P(A) + P(B) - P(A \cup B) $$
$$ P(A \cup B) = P(A) + P(B) - P(A \cap B) $$
$$ P(A | B) = \frac{P(A \cap B)}{P(B)} = P(B|A) = \frac{P(B \cap A)}{P(A)}$$
co daje Twierdzenie Bayesa
$$ P(A | B) = \frac{P(B | A)P(A)}{P(B)} $$

## Klasyfikator pełny Bayesa
$$ P(d | X) = \frac{P(X|d)P(d)}{P(X)} $$

Przypadek 2 klas
$$ P(d_1) \rightarrow P(d_2) = 1 - P(d_1) $$
$$ P(d | X) = \frac{P(X|d) P(d)}{P(X|d_1)P(d) + P(X|\bar{d})P(\bar{d})} $$
Przypadek 3+ klas
$$ P(d_i | X) = \frac{P(X|d_i) P(d_i)}{\displaystyle \sum_{k=1}^{m} P(X|d_i)P(d_i)}$$
Dla niezależnych zmiennych zachodzi
$$P(X_1X_2X_3\cdots X_n) = P(X_1)P(X_2) \cdots P(X_n)$$

*insert dowolny przykład z Bayesem*

## Naiwny klasyfikator Bayesa
Podejmuje decyzje wyłącznie na podstawie licznika ze wzoru Bayesa. Czyli 
$$ P(d_i | X) = \frac{P(X|d_i) P(d_i)}{\sout{\displaystyle \sum_{k=1}^{m} P(X|d_i)P(d_i)}}$$
Przy niezależności $x_i$ tworzących $x$ wzór upraszcza się
$$\mu(d_i|X) = P(X|d_i)P(d_i)$$

A co jeśli $x_i$ jest rzeczywistoliczbowe?
$$\mu(d_i|X) = P(X|d_i)P(d_i)$$
Z $P(d_i)$ nie ma problemu, ale 
$$P(X|d_i) = \frac{1}{(2\pi)^{\frac{N}{2}} * |S_i|}e^{-\frac{1}{2}(x-m_i)^TS_i^{-1}(x-m_i)}$$
gdzie $S_i$ to jest macierz konwoluncji $X$, a $|S_i|$ to jest wyznacnik tej macierzy.

Jeśli $X_i$ są niezależne to wzórj upraszcza się do 
$$P(X|d_i) = Gauss(x_1|d_i)\cdots Gauss(x_n|d_i)$$
$$Gaus(x_k|d_i) = \frac{1}{\sqrt{2\pi \sigma_{ik}^2}}e^{-\frac{1}{2 \sigma_{ik}^2}(x_{ik}-m_{ik})}$$
gdzie $i$ - klasa, $k$ - nr zmiennej w wektorze X, $\sigma_{ik}^2$ wariancja zmiennej $x_k$ w klasie $i$-tej, $m_{ik}$ cov